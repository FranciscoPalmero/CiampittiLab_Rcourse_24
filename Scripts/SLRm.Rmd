
# Packages
```{r message=FALSE}
library("tidyverse")
```


# ------------------------------------------------------------------

# 1) MODEL 1

# 1.1) Reading the dataset for RUE experiment
5 sowing dates by 1 hybrid = 5 treatments by 3 repetitions in two seasons (2022 and 2023)
```{r}
df1 <- read.csv("../Data/RUE.csv") %>% 
  mutate(across(season,as.character))

```

# 1.2) Exploring the data

```{r}
summary(df1)
glimpse(df1)
```



```{r}
df1 %>% 
  ggplot(aes(iPAR_mj.m2,DW_g.m2)) +
  geom_point(aes(shape=season),
             size=3) +
  scale_x_continuous(limits = c(150,480))+
  scale_y_continuous(limits = c(0,1800)) +
  theme_bw()
```

We can assume $$y_i = \beta_0 + \beta_1 x_i + \epsilon_i$$

# 1.3) Getting the least squares estimators

LSE for model parameters 
```{r}
n = nrow(df1)

# LSE for \beta_1
b1_hat = sum((df1$iPAR_mj.m2-mean(df1$iPAR_mj.m2))*(df1$DW_g.m2-mean(df1$DW_g.m2)))/
  sum((df1$iPAR_mj.m2-mean(df1$iPAR_mj.m2))^2)

# LSE for \beta_0
b0_hat = mean(df1$DW_g.m2-b1_hat*mean(df1$iPAR_mj.m2))

# LSE for \sigma^2
sig2_hat = (sum((df1$DW_g.m2-(b0_hat+b1_hat*df1$iPAR_mj.m2))^2))/(n-2)
se_hat = sqrt(sig2_hat)

# LSE for simple linear regression model
b1_hat
b0_hat
sig2_hat

```


Getting the standard error for $\hat{\beta}_0$ and $\hat{\beta}_1$
```{r}
# se(\beta_1)
var_b1hat = sig2_hat/sum((df1$iPAR_mj.m2-mean(df1$iPAR_mj.m2))^2)
se_b1hat = sqrt(var_b1hat)

# se(\beta_1)
var_b0hat = sig2_hat*((1/n)+((mean(df1$iPAR_mj.m2)^2)/sum((df1$iPAR_mj.m2-mean(df1$iPAR_mj.m2))^2)))
se_b0hat = sqrt(var_b0hat)

se_b1hat
se_b0hat
```

# 1.4) Tetsing Hypotheses

Add the assumption the $$y_i \sim N(\beta_0 + \beta_1 x_i, \sigma^2)$$

## 1.4.1) p-value approach

Test $H_0: \beta_1 = 0 \\ \beta_1 \neq 0$
```{r}
# Build the t-statistic for H_0
t_star_b1 = b1_hat/se_b1hat
# Compute p-value = 2*P(t>t_star)
p_val_b1 <- 2*(1-pt(abs(t_star_b1),(n-2)))
p_val_b1
```
Since $p-value<0.05$, we reject $H_0$

Test $H_0: \beta_0 = 0 \\ \beta_0 \neq 0$
```{r}
# Build the t-statistic for H_0
t_star_b0 = b0_hat/se_b0hat
# Compute p-value = 2*P(t>t_star)
p_val_b0 <- 2*(1-pt(abs(t_star_b0),(n-2)))
p_val_b0
```

Since $p-value>0.05$, we do not reject $H_0$


## 1.4.2) Rejection region approach

Test $H_0: \beta_1 = 0 \\ \beta_1 \neq 0$
```{r}
t_star_b1
# Obtain p-value for \alpha=0.05
qt(1-(0.05/2),(n-2))

```
The rejection region is then $t<-2.048407$ or $t>2.048407$. Since $t=8.501$, then we reject the null hypothesis.

Test $H_0: \beta_0 = 0 \\ \beta_0 \neq 0$
```{r}
t_star_b0
# Obtain p-value for \alpha=0.05
qt(1-(0.05/2),(n-2))

```


The rejection region is then $t<-2.048407$ or $t>2.048407$. Since $t=-1.696145$, then we do not reject the null hypothesis.



# 1.5) ANOVA of the Regression

Add the assumption the $$y_i \sim N(\beta_0 + \beta_1 x_i, \sigma^2)$$

## 1.5.1) Geting sum of squares
```{r}
SSR = sum(((b0_hat+b1_hat*df1$iPAR_mj.m2)-mean(df1$DW_g.m2))^2)
SSE = sum((df1$DW_g.m2-(b0_hat+b1_hat*df1$iPAR_mj.m2))^2)
```

## 1.5.2) Geting mean of the squares
```{r}
MSR = SSR/1
MSE = SSE/(n-2)
```

## 1.5.3) Calculate the F-statistic
```{r}
F_star = MSR/MSE

F_star
```

## 1.5.4) Testing hypothesis of the model

$$H_0: y_i = \beta_0 + \epsilon_i \\ H_1: y_i = \beta_0 + \beta_1 x_i + \epsilon_i$$

### 1.4.1) Compute the p-value

```{r}
p_val_Fstar = 1-pf(F_star,1,(n-2))

p_val_Fstar
```


# 1.6) Compute R2

$$R^2 = \frac{SSR}{SST}$$

```{r}
SSR/(SSR+SSE)
```

# 1.7) Summary of the Simple Linear Regression Model

```{r}
tibble(Parameter = c("b0_hat","b1_hat","se_hat"),
       "E[.]" = c(b0_hat,b1_hat,se_hat),
       "se(.)" = c(se_b0hat,se_b1hat,NA),
       t_star = c(t_star_b0,t_star_b1,NA),
       p_value = c(p_val_b0,p_val_b1,NA)) %>% 
  mutate(across(c("E[.]","se(.)","t_star"),round,digits=3))

```


# 1.8) ANOVA of the Model

```{r}
tibble(Source = c("Regression","Error","Total"),
       SS = c(SSR,SSE,(SSR+SSR)),
       df = c("1","n-2=28","n-1=29"),
       MS = c(MSR,MSE,NA),
       F_star = c(round(F_star,2),NA,NA),
       p_val_Fstar = c(p_val_Fstar,NA,NA)) #%>% 
  mutate(across(c("E[.]","se(.)","t_star"),round,digits=3))
```


# 1.9) Taking the advantage of R program
```{r}
lin_model <- lm(DW_g.m2~iPAR_mj.m2, data=df1)

summary(lin_model)

```

# 1.10) Checking the assumptions

The assumptions are always checked by using the residuals

```{r}
res <- residuals(lin_model)
```

## 1.10.1) Normality

$H_0: y_i \sim N(\beta_0+\beta_1x_i+\epsilon_i)$
```{r}
# Shapiro-Wilk Normality Test
shapiro.test(res) # H0: residuals are normally distributed; do not reject H0 if p-value > 0.05
# qqplot
qqnorm(res);qqline(res)
```

## 1.10.2) Homocedasticity or Homegeoneous Variance

$H_0: \sigma^2_1=\sigma^2_2 = ... =\sigma^2_n$

```{r}
# Levene test
plot(fitted(lin_model), res);abline(0,0)

plot(fitted(lin_model),rstandard(lin_model));abline(0,0)

```

# 1.11) Plot the Fitted Model
```{r}
df_fit <- tibble(x=seq(min(df1$iPAR_mj.m2),
             max(df1$iPAR_mj.m2),0.1),
       y = lin_model$coefficients[1]+lin_model$coefficients[2]*x)


df1 %>% 
  ggplot() +
  geom_point(aes(iPAR_mj.m2,DW_g.m2,
                 shape=season),
             size=3,
             fill = "grey95") +
  scale_shape_manual(values=c("2022"=21,
                              "2023"=25)) +
  geom_line(data = df_fit,
            aes(x,y),
            linewidth=1, 
            color = "purple") +
  scale_x_continuous(limits = c(150,480))+
  scale_y_continuous(limits = c(0,1800)) +
  theme_bw()
  

```


# ------------------------------------------------------------------
# 2) MODEL 2

# 2.1) Bringing the data 
```{r}
df2 <- read.csv("../Data/Nfer_Corn.csv") %>% 
  mutate(across(block,as.character))
```


# 2.2) Exploring the data

```{r}
summary(df2)
glimpse(df2)
```



```{r}
df2 %>% 
  ggplot(aes(Nrate_kg.ha,GY_kg.ha)) +
  geom_point(aes(color=block),
             size=3) +
  scale_x_continuous(limits = c(0,380))+
  scale_y_continuous(limits = c(0,14000)) +
  theme_bw()
```

# 2.3) Fit the model

$$y_i = \beta_0+\beta_1x_i+\beta_2 x_i^2 + \epsilon_i$$

Assumption

$$y_i \overset{\text{ind.}}{\sim}  N(\beta_0+\beta_1x_i+\beta_2 x_i^2, ~\sigma^2) $$


```{r}
lin_mod <- lm(GY_kg.ha~Nrate_kg.ha, data = df2)
quad_mod <- lm(GY_kg.ha~Nrate_kg.ha+I(Nrate_kg.ha^2), data = df2)

summary(quad_mod)

# anova(lin_mod,quad_mod)

```

```{r}
contrasts()
```


# 2.4) Checking assumptions

The assumptions are always checked by using the residuals

```{r}
res <- residuals(quad_mod)
```

## 2.4.1) Normality

$H_0: y_i \sim N(\beta_0+\beta_1x_i+\epsilon_i)$
```{r}
# Shapiro-Wilk Normality Test
shapiro.test(res) # H0: residuals are normally distributed; do not reject H0 if p-value > 0.05
# qqplot
qqnorm(res);qqline(res)
```

##  2.4.2) Homocedasticity or Homegeoneous Variance

$H_0: \sigma^2_1=\sigma^2_2 = ... =\sigma^2_n$

```{r}
# Levene test
plot(fitted(quad_mod), res);abline(0,0)

plot(fitted(quad_mod),rstandard(quad_mod));abline(0,0)

```


# 2.5) Plot the Fitted Model
```{r}
df_fit <- tibble(x=seq(min(df2$Nrate_kg.ha),
             max(df2$Nrate_kg.ha),1),
       y = quad_mod$coefficients[1]+quad_mod$coefficients[2]*x+quad_mod$coefficients[3]*(x^2))



df2 %>% 
  ggplot(aes(Nrate_kg.ha,GY_kg.ha)) +
  geom_point(aes(fill=block),
             size=3,
             shape = 21,
             alpha = 0.8) +
  geom_line(data = df_fit,
            aes(x,y),
            linewidth=1, 
            color = "purple") +
  scale_x_continuous(limits = c(0,380))+
  scale_y_continuous(limits = c(0,14000)) +
  theme_bw()
```





